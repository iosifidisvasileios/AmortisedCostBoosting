{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from AdaCC import AdaCC\n",
    "from Competitors.CGAda_Cal import CGAda_Cal\n",
    "from Competitors.AdaMEC_Cal import AdaMEC_Cal\n",
    "from Competitors.RareBoost import RareBoost\n",
    "from Competitors.AdaMEC import AdaMEC\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from Competitors.CostBoostingAlgorithms import CostSensitiveAlgorithms\n",
    "from collections import defaultdict, Counter\n",
    "import operator, os, pickle\n",
    "from multiprocessing import Process\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "from imblearn.datasets import make_imbalance\n",
    "from sklearn.datasets import make_moons, make_blobs, make_circles, make_classification\n",
    "from sklearn.metrics import f1_score, balanced_accuracy_score, recall_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# X, y = make_moons(n_samples=150, noise=7/ 10., random_state=10)      \n",
    "\n",
    "\n",
    "def plot_decision_boundary(classifier, X, y, N = 10, scatter_weights = None , ax = None, flag=False, second=False ):\n",
    "    if scatter_weights is None:\n",
    "         scatter_weights = np.ones(len(y))\n",
    "    '''Utility function to plot decision boundary and scatter plot of data'''\n",
    "    x_min, x_max = X[:, 0].min() - .1, X[:, 0].max() + .1\n",
    "    y_min, y_max = X[:, 1].min() - .1, X[:, 1].max() + .1\n",
    "    xx, yy = np.meshgrid( np.linspace(x_min, x_max, N), np.linspace(y_min, y_max, N))\n",
    "\n",
    "    zz = np.array( [classifier.predict_proba(np.array([xi,yi]).reshape(1,-1))[:,1] for  xi, yi in zip(np.ravel(xx), np.ravel(yy)) ] )\n",
    "\n",
    "#     #Check what methods are available\n",
    "    if hasattr(classifier, \"decision_function\"):\n",
    "        zz = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        #         if flag and not second:\n",
    "#             zz = np.array( [classifier.decision_function(np.array([xi,yi]).reshape(1,-1))[:,1] for  xi, yi in zip(np.ravel(xx), np.ravel(yy)) ] )\n",
    "#         elif flag and second:\n",
    "#             zz = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "            \n",
    "#         elif not flag:\n",
    "#             zz = np.array( [classifier.decision_function(np.array([xi,yi]).reshape(1,-1)) for  xi, yi in zip(np.ravel(xx), np.ravel(yy)) ] )\n",
    "        \n",
    "            #         print(\"decision_function\")\n",
    "    elif hasattr(classifier, \"predict_proba\"):\n",
    "        zz = np.array( [classifier.predict_proba(np.array([xi,yi]).reshape(1,-1))[:,1] for  xi, yi in zip(np.ravel(xx), np.ravel(yy)) ] )\n",
    "    \n",
    "    \n",
    "    if not flag:\n",
    "        zz = np.array( [classifier.predict_proba(np.array([xi,yi]).reshape(1,-1))[:,1] for  xi, yi in zip(np.ravel(xx), np.ravel(yy)) ] )\n",
    "\n",
    "        #         print(\"predict_proba\")\n",
    "#     else :\n",
    "#         print(\"none\")\n",
    "\n",
    "#         zz = np.array( [classifier(np.array([xi,yi]).reshape(1,-1)) for  xi, yi in zip(np.ravel(xx), np.ravel(yy)) ] )\n",
    "    \n",
    "    # reshape result and plot\n",
    "    Z = zz.reshape(xx.shape)\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    \n",
    "    #Get current axis and plot\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    ax.contourf(xx, yy, Z, 2, cmap='RdBu', alpha=.7,levels=[0,0.5,1])\n",
    "    ax.scatter(X[:,0],X[:,1], c = y, cmap = cm_bright, s = scatter_weights * 40)\n",
    "#     ax.set_xlabel('$X_1$')\n",
    "#     ax.set_ylabel('$X_2$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def AdaCC_scratch(X,y, M, method):\n",
    "    #Initialization of utility variables\n",
    "    N = len(y)\n",
    "    estimator_list, y_predict_list, estimator_error_list, estimator_weight_list, sample_weight_list = [], [],[],[],[]\n",
    "  \n",
    "    adacc = AdaCC(n_estimators=M, algorithm=method)\n",
    "    adacc.fit(X, y)\n",
    "\n",
    "    #Save iteration values\n",
    "    estimator_list = adacc.estimator_tmp\n",
    "    estimator_weight_list = adacc.alpha_tmp\n",
    "    sample_weight_list = adacc.smp_w_tmp\n",
    "        \n",
    "\n",
    "\n",
    "    #Convert to np array for convenience   \n",
    "    estimator_list = np.asarray(estimator_list)\n",
    "    estimator_weight_list = np.asarray(estimator_weight_list)\n",
    "    sample_weight_list = np.asarray(sample_weight_list)\n",
    "     \n",
    "    return estimator_list, estimator_weight_list, sample_weight_list, adacc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def competitors(X,y, M, method):\n",
    "    N = len(y)\n",
    "    estimator_list, y_predict_list, estimator_error_list, estimator_weight_list, sample_weight_list = [], [],[],[],[]\n",
    "    \n",
    "    def train_model(X_train,y_train, base_learners, method):\n",
    "        \n",
    "        def train_competitors(X_train, y_train, base_learners, method, maj, min, ratio):\n",
    "            try:\n",
    "                out = []\n",
    "                if method == 'CGAda_Cal':\n",
    "                    clf = CGAda_Cal(n_estimators=base_learners, algorithm=method, class_weight={min: 1, maj: ratio / 10.})\n",
    "                else:\n",
    "                    clf = CostSensitiveAlgorithms(n_estimators=base_learners, algorithm=method,\n",
    "                                                  class_weight={min: 1, maj: ratio / 10.})\n",
    "\n",
    "                clf.fit(X_train, y_train)\n",
    "\n",
    "                out.append(f1_score(y_train, clf.predict(X_train)))\n",
    "                out.append(clf)\n",
    "                with open('temp_preds/' + method + str(ratio), 'wb') as filehandle:\n",
    "                    pickle.dump(out, filehandle)\n",
    "            except:\n",
    "                return\n",
    "\n",
    "        \n",
    "        \n",
    "        if 'AdaMEC_Cal' in method:\n",
    "            counter_dict = Counter(list(y_train))\n",
    "            majority = max(counter_dict.items(), key=operator.itemgetter(1))[0]\n",
    "            minority = max(counter_dict.items(), key=operator.itemgetter(0))[0]\n",
    "            ratios = [1., 2., 3., 4., 5., 6., 7, 8., 9., 10.]\n",
    "            clf = AdaMEC_Cal(n_estimators=base_learners, algorithm=method)\n",
    "            clf.fit(X_train, y_train)\n",
    "            best_score = -1\n",
    "            best_idx = 0\n",
    "            for idx, cost in enumerate(ratios):\n",
    "                class_weight = {minority: 1, majority: cost / 10.}\n",
    "                clf.set_costs(y_train, class_weight)\n",
    "                score = f1_score(y_train, clf.predict(X_train))\n",
    "                if best_score < score:\n",
    "                    best_idx = idx\n",
    "                    best_score = score\n",
    "            class_weight = {minority: 1, majority: ratios[best_idx] / 10.}\n",
    "            clf.set_costs(y_train, class_weight)\n",
    "\n",
    "\n",
    "        elif 'AdaMEC' in method:\n",
    "            counter_dict = Counter(list(y_train))\n",
    "            majority = max(counter_dict.items(), key=operator.itemgetter(1))[0]\n",
    "            minority = max(counter_dict.items(), key=operator.itemgetter(0))[0]\n",
    "            ratios = [1., 2., 3., 4., 5., 6., 7, 8., 9., 10.]\n",
    "            clf = AdaMEC(n_estimators=base_learners, algorithm=method)\n",
    "            clf.fit(X_train, y_train)\n",
    "            best_score = -1\n",
    "            best_idx = 0\n",
    "            for idx, cost in enumerate(ratios):\n",
    "                class_weight = {minority: 1, majority: cost / 10.}\n",
    "                clf.set_costs(class_weight)\n",
    "                score = f1_score(y_train, clf.predict(X_train))\n",
    "                if best_score < score:\n",
    "                    best_idx = idx\n",
    "                    best_score = score\n",
    "            class_weight = {minority: 1, majority: ratios[best_idx] / 10.}\n",
    "            clf.set_costs(class_weight)\n",
    "\n",
    "        elif method == 'RareBoost':\n",
    "            clf = RareBoost(n_estimators=base_learners)\n",
    "            clf.fit(X_train, y_train)\n",
    "        else:\n",
    "            counter_dict = Counter(list(y_train))\n",
    "            majority = max(counter_dict.items(), key=operator.itemgetter(1))[0]\n",
    "            minority = max(counter_dict.items(), key=operator.itemgetter(0))[0]\n",
    "            ratios = [1., 2., 3., 4., 5., 6., 7, 8., 9., 10.]\n",
    "            for ratio in ratios:\n",
    "                train_competitors(X_train, y_train,   base_learners, method, majority, minority, ratio)\n",
    "                \n",
    "            best_score = -1\n",
    "            best_ratio = 0\n",
    "            for ratio in ratios:\n",
    "                if os.path.exists('temp_preds/' + method + str(ratio)):\n",
    "                    with open('temp_preds/' + method + str(ratio), 'rb') as filehandle:\n",
    "                        temp = pickle.load(filehandle)\n",
    "                        if temp[0] > best_score:\n",
    "                            best_ratio = ratio\n",
    "                            best_score = temp[0]\n",
    "                            clf = temp[1]\n",
    "\n",
    "#                 if os.path.exists('temp_preds/' + method + str(ratio)):\n",
    "#                     os.remove('temp_preds/' + method + str(ratio))\n",
    "\n",
    "        return clf\n",
    "    \n",
    "    print(method)\n",
    "    clf = train_model(X,y, M, method)\n",
    "    \n",
    "    if method == \"CGAda_Cal\" or  method == \"AdaMEC\" or  method == \"AdaMEC_Cal\":\n",
    "        return _,_,clf\n",
    "    estimator_list = clf.estimator_tmp\n",
    "    sample_weight_list = clf.smp_w_tmp\n",
    "    \n",
    "    #Convert to np array for convenience   \n",
    "    estimator_list = np.asarray(estimator_list)\n",
    "    sample_weight_list = np.asarray(sample_weight_list)\n",
    "     \n",
    "    return estimator_list, sample_weight_list, clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def AdaBoost_scratch(X,y, M=10, learning_rate = 1):\n",
    "    #Initialization of utility variables\n",
    "    N = len(y)\n",
    "    estimator_list, y_predict_list, estimator_error_list, estimator_weight_list, sample_weight_list = [], [],[],[],[]\n",
    "\n",
    "    #Initialize the sample weights\n",
    "    sample_weight = np.ones(N) / float(N)\n",
    "    sample_weight_list.append(sample_weight.copy())\n",
    "\n",
    "    #For m = 1 to M\n",
    "    for m in range(M):   \n",
    "\n",
    "        #Fit a classifier\n",
    "        estimator = DecisionTreeClassifier(max_depth = 1, max_leaf_nodes=2)\n",
    "        estimator.fit(X, y, sample_weight=sample_weight)\n",
    "        y_predict = estimator.predict(X)\n",
    "\n",
    "        #Misclassifications\n",
    "        incorrect = (y_predict != y)\n",
    "\n",
    "        #Estimator error\n",
    "        estimator_error = np.mean( np.average(incorrect, weights=sample_weight, axis=0))\n",
    "        \n",
    "        #Boost estimator weights\n",
    "        estimator_weight =  learning_rate * np.log((1. - estimator_error) / estimator_error)\n",
    "\n",
    "        #Boost sample weights\n",
    "        sample_weight *= np.exp(estimator_weight * incorrect * ((sample_weight > 0) | (estimator_weight < 0)))\n",
    "\n",
    "        #Save iteration values\n",
    "        estimator_list.append(estimator)\n",
    "        y_predict_list.append(y_predict.copy())\n",
    "        estimator_error_list.append(estimator_error.copy())\n",
    "        estimator_weight_list.append(estimator_weight.copy())\n",
    "        sample_weight_list.append(sample_weight.copy())\n",
    "        \n",
    "    #Convert to np array for convenience   \n",
    "    estimator_list = np.asarray(estimator_list)\n",
    "    estimator_weight_list = np.asarray(estimator_weight_list)\n",
    "    sample_weight_list = np.asarray(sample_weight_list)\n",
    "\n",
    "    return estimator_list, estimator_weight_list, sample_weight_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaMEC\n",
      "AdaMEC_Cal\n",
      "CGAda\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'clf' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-3fdf7ea37fc7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mestimators_temp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight_temp\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mAdaBoost_scratch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m             \u001b[0mestimators_temp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight_temp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mcompetitors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbalanced_accuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-80381cbc2c20>\u001b[0m in \u001b[0;36mcompetitors\u001b[1;34m(X, y, M, method)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m     \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"CGAda_Cal\"\u001b[0m \u001b[1;32mor\u001b[0m  \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"AdaMEC\"\u001b[0m \u001b[1;32mor\u001b[0m  \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"AdaMEC_Cal\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-80381cbc2c20>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(X_train, y_train, base_learners, method)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;31m#                     os.remove('temp_preds/' + method + str(ratio))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'clf' referenced before assignment"
     ]
    }
   ],
   "source": [
    "M =5\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "for loop in range (414,416):\n",
    "    X, y = make_circles(n_samples = 200, noise=0.3, factor=0.3,random_state=loop)\n",
    "    X, y = make_imbalance(X, y, sampling_strategy={0: 30, 1: 10},random_state=0)\n",
    "    \n",
    "    \n",
    "    list_of_methods = ['AdaBoost', 'AdaCC1', 'AdaCC2',  'AdaMEC', 'AdaMEC_Cal', 'CGAda',\n",
    "                       'CGAda_Cal', 'AdaCost', 'CSB1',\n",
    "                       'CSB2', 'AdaC1', 'AdaC2', 'AdaC3', 'RareBoost']\n",
    "    \n",
    "    temp_methods = list(list_of_methods)\n",
    "    \n",
    "    estimator_list = []\n",
    "    sample_weight_list = []\n",
    "    models = []\n",
    "    scores = []\n",
    "    for method in list_of_methods:\n",
    "        if \"AdaCC\" in method:\n",
    "            estimators_temp, _, sample_weight_temp, model  = AdaCC_scratch(X,y, M, 'AdaCC1')\n",
    "        elif method == \"AdaBoost\":\n",
    "            model = AdaBoostClassifier( base_estimator = DecisionTreeClassifier(max_depth = 1, max_leaf_nodes=2), algorithm = 'SAMME',n_estimators=M, learning_rate=1.0)\n",
    "            model.fit(X,y)\n",
    "            estimators_temp, _, sample_weight_temp  = AdaBoost_scratch(X,y, M, learning_rate = 1)\n",
    "        else:\n",
    "            estimators_temp, sample_weight_temp, model  = competitors(X,y, M, method)\n",
    "\n",
    "        scores.append(balanced_accuracy_score(y,model.predict(X)))\n",
    "\n",
    "        if method != \"AdaMEC\" and method != \"AdaMEC_Cal\" and method != \"CGAda_Cal\":\n",
    "            estimator_list.append(estimators_temp)\n",
    "            sample_weight_list.append(sample_weight_temp)\n",
    "        elif method == \"AdaMEC\" or method == \"AdaMEC_Cal\":\n",
    "            estimator_list.append(estimator_list[0])\n",
    "            sample_weight_list.append(sample_weight_list[0])\n",
    "            \n",
    "        elif method == \"CGAda_Cal\":\n",
    "            estimator_list.append(estimator_list[5])\n",
    "            sample_weight_list.append(sample_weight_list[5])\n",
    "        models.append(model)\n",
    "        \n",
    "        \n",
    "    zipped_list = zip(scores, temp_methods)\n",
    "    sorted_pairs = sorted(zipped_list, reverse=True)\n",
    "\n",
    "    if 'AdaCC' in sorted_pairs[0][1] and 'AdaCC' in  sorted_pairs[1][1]:\n",
    "        go = True\n",
    "        print (loop)\n",
    "    else:\n",
    "        go = False    \n",
    "      \n",
    "    if go:\n",
    "        fig, axs = plt.subplots(len(list_of_methods), M+1, figsize = (22,30),  sharex='col', sharey='row')      \n",
    "        for k in range(0, len(list_of_methods)):\n",
    "            for m in range(0,M+1):\n",
    "                if m != M:\n",
    "                    s_weights = (sample_weight_list[k][m,:] / sample_weight_list[k][m,:].sum() ) * len(y)\n",
    "                    plot_decision_boundary(estimator_list[k][m], X,y,N = 50,ax = axs[k,m], scatter_weights =s_weights )\n",
    "                    if k == 0:\n",
    "                        axs[k,m].set_title('Estimator {} boundary'.format(m+1))\n",
    "                else:\n",
    "                    axs[k,0].set_ylabel(list_of_methods[k])\n",
    "                    s_weights = (sample_weight_list[k][0,:] / sample_weight_list[k][0,:].sum() ) * len(y)\n",
    "                    flag =True\n",
    "                    flag2 = False\n",
    "                    if list_of_methods[k] == \"AdaBoost\":\n",
    "                        flag = False\n",
    "                    \n",
    "                    if \"AdaMEC\" in list_of_methods[k] or \"Cal\" in list_of_methods[k] :\n",
    "                        flag2 = True\n",
    "                    \n",
    "                    print(list_of_methods[k])\n",
    "                    plot_decision_boundary(models[k], X,y,N = 50, ax = axs[k,m], flag=flag, second=flag2)\n",
    "                    if k == 0:\n",
    "                        axs[k,m].set_title('Ensemble boundary')\n",
    "\n",
    "            plt.tight_layout()\n",
    "\n",
    "        plt.savefig(\"Images/Boundaries/overall\" + str(loop) + \".png\", bbox_inches='tight', dpi=100)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
